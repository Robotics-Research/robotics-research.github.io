[
  {
    "id": 1,
    "title": "GhostImage: Remote Perception Attacks against Camera-based Image Classification Systems",
    "year": "21 Jan 2020",
    "summary": "In vision-based object classification systems imaging sensors perceive the environment and machine learning is then used to detect and classify objects for decision-making purposes; e.g., to maneuver an automated vehicle around an obstacle or to raise an alarm to indicate the presence of an intruder in surveillance settings. In this work we demonstrate how the perception domain can be remotely and unobtrusively exploited to enable an attacker to create spurious objects or alter an existing object.",
    "sources": [
      {
        "type": "paper",
        "title": "GhostImage: Remote Perception Attacks against Camera-based Image Classification Systems",
        "url": "https://arxiv.org/abs/2001.07792v3"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2001.07792v3"
      },
      {
        "type": "code",
        "title": "Source code",
        "url": "https://github.com/Harry1993/GhostImage"
      },
      {
        "type": "article",
        "title": "Phantom of the ADAS",
        "url": "https://www.nassiben.com/phantoms"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Projector Attack",
      "Camera Sensor Spoofing",
      "Autonomous Driving"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 2,
    "title": "Adversarial Imaging Pipelines",
    "year": "07 Feb 2021",
    "summary": "Adversarial attacks play an essential role in understanding deep neural network predictions and improving their robustness. Existing attack methods aim to deceive convolutional neural network (CNN)-based classifiers by manipulating RGB images that are fed directly to the classifiers.",
    "sources": [
      {
        "type": "paper",
        "title": "Adversarial Imaging Pipelines",
        "url": "https://arxiv.org/abs/2102.03728v2"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2102.03728v2"
      },
      {
        "type": "website",
        "title": "Adversarial Imaging Pipelines project page",
        "url": "https://light.princeton.edu/publication/adversarial-pipelines/"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Camera ISP Manipulation",
      "Adversarial Example Generation"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 3,
    "title": "Adversarial Sticker: A Stealthy Attack Method in the Physical World",
    "year": "14 Apr 2021",
    "summary": "To assess the vulnerability of deep learning in the physical world, recent works introduce adversarial patches and apply them on different tasks. In this paper, we propose another kind of adversarial patch: the Meaningful Adversarial Sticker, a physically feasible and stealthy attack method by using real stickers existing in our life.",
    "sources": [
      {
        "type": "paper",
        "title": "Adversarial Sticker: A Stealthy Attack Method in the Physical World",
        "url": "https://arxiv.org/abs/2104.06728v2"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2104.06728v2"
      },
      {
        "type": "journal",
        "title": "IEICE Transactions publication",
        "url": "https://ieice.org/en_transactions/information/10.1587/transinf.2022MUL0001/"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Adversarial Patch",
      "Sticker Attack",
      "Traffic Sign Spoofing"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 4,
    "title": "DoubleStar: Long-Range Attack Towards Depth Estimation based Obstacle Avoidance in Autonomous Systems",
    "year": "07 Oct 2021",
    "summary": "Depth estimation-based obstacle avoidance has been widely adopted by autonomous systems (drones and vehicles) for safety purpose. It normally relies on a stereo camera to automatically detect obstacles and make flying/driving decisions, e.g., stopping several meters ahead of the obstacle in the path or moving away from the detected obstacle.",
    "sources": [
      {
        "type": "paper",
        "title": "DoubleStar: Long-Range Attack Towards Depth Estimation based Obstacle Avoidance in Autonomous Systems",
        "url": "https://arxiv.org/abs/2110.03154v1"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2110.03154v1"
      },
      {
        "type": "website",
        "title": "DoubleStar project site",
        "url": "https://fakedepth.github.io/"
      },
      {
        "type": "talk",
        "title": "USENIX Security 2022 presentation",
        "url": "https://www.usenix.org/conference/usenixsecurity22/presentation/zhou-ce"
      },
      {
        "type": "pdf",
        "title": "USENIX Security 2022 paper",
        "url": "https://www.usenix.org/system/files/sec22-zhou-ce.pdf"
      },
      {
        "type": "article",
        "title": "On the Realism of LiDAR Spoofing Attacks",
        "url": "https://www.ndss-symposium.org/ndss-paper/on-the-realism-of-lidar-spoofing-attacks/"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Optical Injection",
      "Depth Estimation",
      "Drone Safety"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 5,
    "title": "Rolling Colors: Adversarial Laser Exploits against Traffic Light Recognition",
    "year": "06 Apr 2022",
    "summary": "Traffic light recognition is essential for fully autonomous driving in urban areas. In this paper, we investigate the feasibility of fooling traffic light recognition mechanisms by shedding laser interference on the camera.",
    "sources": [
      {
        "type": "paper",
        "title": "Rolling Colors: Adversarial Laser Exploits against Traffic Light Recognition",
        "url": "https://arxiv.org/abs/2204.02675v1"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2204.02675v1"
      },
      {
        "type": "slides",
        "title": "Rolling Colors slides",
        "url": "https://cyansec.com/files/slides/22SEC_RollingColors-slides.pdf"
      },
      {
        "type": "paper",
        "title": "Adversarial Laser Spot: Robust and Covert Physical-World Attack to DNNs",
        "url": "https://proceedings.mlr.press/v189/hu23b.html"
      },
      {
        "type": "pdf",
        "title": "Adversarial Laser Spot PDF",
        "url": "https://proceedings.mlr.press/v189/hu23b/hu23b.pdf"
      },
      {
        "type": "pdf",
        "title": "USENIX Security 2022 Summer paper",
        "url": "https://www.usenix.org/system/files/sec22summer_yan.pdf"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Laser Injection",
      "Traffic Light Spoofing",
      "Autonomous Driving"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 6,
    "title": "Robust Contrastive Language-Image Pre-training against Data Poisoning and Backdoor Attacks",
    "year": "13 Mar 2023",
    "summary": "Contrastive vision-language representation learning has achieved state-of-the-art performance for zero-shot classification, by learning from millions of image-caption pairs crawled from the internet. However, the massive data that powers large multimodal models such as CLIP, makes them extremely vulnerable to various types of targeted data poisoning and backdoor attacks.",
    "sources": [
      {
        "type": "paper",
        "title": "Robust Contrastive Language-Image Pre-training against Data Poisoning and Backdoor Attacks",
        "url": "https://arxiv.org/abs/2303.06854v2"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2303.06854v2"
      }
    ],
    "tags": [
      "Defense",
      "Data Poisoning Mitigation",
      "Backdoor Defense",
      "Vision-Language Models"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 7,
    "title": "State-of-the-art optical-based physical adversarial attacks for deep learning computer vision systems",
    "year": "22 Mar 2023",
    "summary": "Adversarial attacks can mislead deep learning models to make false predictions by implanting small perturbations to the original input that are imperceptible to the human eye, which poses a huge security threat to the computer vision systems based on deep learning. Physical adversarial attacks, which is more realistic, as the perturbation is introduced to the input before it is being captured and converted to a binary image inside the vision system, when compared to digital adversarial attacks.",
    "sources": [
      {
        "type": "paper",
        "title": "State-of-the-art optical-based physical adversarial attacks for deep learning computer vision systems",
        "url": "https://arxiv.org/abs/2303.12249v1"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2303.12249v1"
      },
      {
        "type": "code",
        "title": "RFLA reflected light attack code",
        "url": "https://github.com/winterwindwang/RFLA"
      }
    ],
    "tags": [
      "Survey",
      "Physical Adversarial",
      "Optical Attack",
      "Threat Modeling"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 8,
    "title": "Imperceptible CMOS camera dazzle for adversarial attacks on deep neural networks",
    "year": "22 Oct 2023",
    "summary": "Despite the outstanding performance of deep neural networks, they are vulnerable to adversarial attacks. While there are many invisible attacks in the digital domain, most physical world adversarial attacks are visible.",
    "sources": [
      {
        "type": "paper",
        "title": "Imperceptible CMOS camera dazzle for adversarial attacks on deep neural networks",
        "url": "https://arxiv.org/abs/2311.16118v1"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2311.16118v1"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Optical Dazzle",
      "Sensor Jamming",
      "Rolling Shutter Exploit"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 9,
    "title": "Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models",
    "year": "05 Feb 2024",
    "summary": "Vision-Language Models (VLMs) excel in generating textual responses from visual inputs, but their versatility raises security concerns. This study takes the first step in exposing VLMs' susceptibility to data poisoning attacks that can manipulate responses to innocuous, everyday prompts.",
    "sources": [
      {
        "type": "paper",
        "title": "Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models",
        "url": "https://arxiv.org/abs/2402.06659v2"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2402.06659v2"
      },
      {
        "type": "code",
        "title": "Source code",
        "url": "https://github.com/umd-huang-lab/VLM-Poisoning"
      },
      {
        "type": "code",
        "title": "ShadowAttack code",
        "url": "https://github.com/hncszya/ShadowAttack"
      }
    ],
    "tags": [
      "Data Poisoning",
      "Vision-Language Models",
      "Misinformation",
      "Stealth Attack"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 10,
    "title": "On the Vulnerability of LLM/VLM-Controlled Robotics",
    "year": "15 Feb 2024",
    "summary": "In this work, we highlight vulnerabilities in robotic systems integrating large language models (LLMs) and vision-language models (VLMs) due to input modality sensitivities. While LLM/VLM-controlled robots show impressive performance across various tasks, their reliability under slight input variations remains underexplored yet critical.",
    "sources": [
      {
        "type": "paper",
        "title": "On the Vulnerability of LLM/VLM-Controlled Robotics",
        "url": "https://arxiv.org/abs/2402.10340v5"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2402.10340v5"
      }
    ],
    "tags": [
      "Robotics",
      "Input Perturbation",
      "Vision-Language Models",
      "Reliability"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 11,
    "title": "Physical Backdoor: Towards Temperature-based Backdoor Attacks in the Physical World",
    "year": "30 Apr 2024",
    "summary": "Backdoor attacks have been well-studied in visible light object detection (VLOD) in recent years. However, VLOD can not effectively work in dark and temperature-sensitive scenarios.",
    "sources": [
      {
        "type": "paper",
        "title": "Physical Backdoor: Towards Temperature-based Backdoor Attacks in the Physical World",
        "url": "https://arxiv.org/abs/2404.19417v1"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2404.19417v1"
      }
    ],
    "tags": [
      "Physical Backdoor",
      "Thermal Imaging",
      "Temperature Trigger",
      "Object Detection"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 12,
    "title": "Adversarial Robustness for Visual Grounding of Multimodal Large Language Models",
    "year": "16 May 2024",
    "summary": "Multi-modal Large Language Models (MLLMs) have recently achieved enhanced performance across various vision-language tasks including visual grounding capabilities. However, the adversarial robustness of visual grounding remains unexplored in MLLMs.",
    "sources": [
      {
        "type": "paper",
        "title": "Adversarial Robustness for Visual Grounding of Multimodal Large Language Models",
        "url": "https://arxiv.org/abs/2405.09981v1"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2405.09981v1"
      },
      {
        "type": "openreview",
        "title": "OpenReview discussion",
        "url": "https://openreview.net/forum?id=Fz0uYu6bZn"
      }
    ],
    "tags": [
      "Vision-Language Models",
      "Visual Grounding",
      "Adversarial Perturbation",
      "Multimodal Attacks"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 13,
    "title": "Safeguarding Vision-Language Models Against Patched Visual Prompt Injectors",
    "year": "17 May 2024",
    "summary": "Large language models have become increasingly prominent, also signaling a shift towards multimodality as the next frontier in artificial intelligence, where their embeddings are harnessed as prompts to generate textual content. Vision-language models (VLMs) stand at the forefront of this advancement, offering innovative ways to combine visual and textual data for enhanced understanding and interaction.",
    "sources": [
      {
        "type": "paper",
        "title": "Safeguarding Vision-Language Models Against Patched Visual Prompt Injectors",
        "url": "https://arxiv.org/abs/2405.10529v2"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2405.10529v2"
      },
      {
        "type": "openreview",
        "title": "OpenReview discussion",
        "url": "https://openreview.net/forum?id=2r8n6kKNEXNa"
      }
    ],
    "tags": [
      "Defense",
      "Adversarial Patch Mitigation",
      "Prompt Injection Defense",
      "Vision-Language Models"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 14,
    "title": "Invisible Optical Adversarial Stripes on Traffic Sign against Autonomous Vehicles",
    "year": "10 Jul 2024",
    "summary": "Camera-based computer vision is essential to autonomous vehicle's perception. This paper presents an attack that uses light-emitting diodes and exploits the camera's rolling shutter effect to create adversarial stripes in the captured images to mislead traffic sign recognition.",
    "sources": [
      {
        "type": "paper",
        "title": "Invisible Optical Adversarial Stripes on Traffic Sign against Autonomous Vehicles",
        "url": "https://arxiv.org/abs/2407.07510v1"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2407.07510v1"
      },
      {
        "type": "pdf",
        "title": "GhostStripe MobiSys PDF",
        "url": "https://tanrui.github.io/pub/GhostStripe-MobiSys.pdf"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Optical Injection",
      "Traffic Sign Spoofing",
      "Autonomous Driving"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 15,
    "title": "Prompt Injection Attacks on Large Language Models in Oncology",
    "year": "23 Jul 2024",
    "summary": "Vision-language artificial intelligence models (VLMs) possess medical knowledge and can be employed in healthcare in numerous ways, including as image interpreters, virtual scribes, and general decision support systems. However, here, we demonstrate that current VLMs applied to medical tasks exhibit a fundamental security flaw: they can be attacked by prompt injection attacks, which can be used to output harmful information just by interacting with the VLM, without any access to its parameters.",
    "sources": [
      {
        "type": "paper",
        "title": "Prompt Injection Attacks on Large Language Models in Oncology",
        "url": "https://arxiv.org/abs/2407.18981v1"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2407.18981v1"
      },
      {
        "type": "journal",
        "title": "Nature Communications article",
        "url": "https://www.nature.com/articles/s41467-024-55631-x"
      }
    ],
    "tags": [
      "Prompt Injection",
      "Healthcare AI",
      "Vision-Language Models",
      "Safety Risk"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 16,
    "title": "Empirical Analysis of Large Vision-Language Models against Goal Hijacking via Visual Prompt Injection",
    "year": "07 Aug 2024",
    "summary": "We explore visual prompt injection (VPI) that maliciously exploits the ability of large vision-language models (LVLMs) to follow instructions drawn onto the input image. We propose a new VPI method, \"goal hijacking via visual prompt injection\" (GHVPI), that swaps the execution task of LVLMs from an original task to an alternative task designated by an attacker.",
    "sources": [
      {
        "type": "paper",
        "title": "Empirical Analysis of Large Vision-Language Models against Goal Hijacking via Visual Prompt Injection",
        "url": "https://arxiv.org/abs/2408.03554v1"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2408.03554v1"
      },
      {
        "type": "blog",
        "title": "GPT-4 vision prompt injection overview",
        "url": "https://blog.roboflow.com/gpt-4-vision-prompt-injection/"
      },
      {
        "type": "blog",
        "title": "Lakera guide to visual prompt injections",
        "url": "https://www.lakera.ai/blog/visual-prompt-injections"
      }
    ],
    "tags": [
      "Visual Prompt Injection",
      "Goal Hijacking",
      "Vision-Language Models",
      "Instruction Following Exploit"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 17,
    "title": "TrojVLM: Backdoor Attack Against Vision Language Models",
    "year": "28 Sep 2024",
    "summary": "The emergence of Vision Language Models (VLMs) is a significant advancement in integrating computer vision with Large Language Models (LLMs) to produce detailed text descriptions based on visual inputs, yet it introduces new security vulnerabilities. Unlike prior work that centered on single modalities or classification tasks, this study introduces TrojVLM, the first exploration of backdoor attacks aimed at VLMs engaged in complex image-to-text generation.",
    "sources": [
      {
        "type": "paper",
        "title": "TrojVLM: Backdoor Attack Against Vision Language Models",
        "url": "https://arxiv.org/abs/2409.19232v1"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2409.19232v1"
      }
    ],
    "tags": [
      "Backdoor Attack",
      "Vision-Language Models",
      "Image Captioning",
      "Trigger Phrases"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 18,
    "title": "Attacking Vision-Language Computer Agents via Pop-ups",
    "year": "04 Nov 2024",
    "summary": "Autonomous agents powered by large vision and language models (VLM) have demonstrated significant potential in completing daily computer tasks, such as browsing the web to book travel and operating desktop software, which requires agents to understand these interfaces. Despite such visual inputs becoming more integrated into agentic applications, what types of risks and attacks exist around them still remain unclear.",
    "sources": [
      {
        "type": "paper",
        "title": "Attacking Vision-Language Computer Agents via Pop-ups",
        "url": "https://arxiv.org/abs/2411.02391v2"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2411.02391v2"
      },
      {
        "type": "code",
        "title": "PopupAttack code",
        "url": "https://github.com/SALT-NLP/PopupAttack"
      }
    ],
    "tags": [
      "UI Manipulation",
      "Vision-Language Agents",
      "Adversarial Pop-ups",
      "Task Hijacking"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 19,
    "title": "TrojanRobot: Physical-world Backdoor Attacks Against VLM-based Robotic Manipulation",
    "year": "18 Nov 2024",
    "summary": "Robotic manipulation in the physical world is increasingly empowered by large language models (LLMs) and vision-language models (VLMs), leveraging their understanding and perception capabilities. Recently, various attacks against such robotic policies have been proposed, with backdoor attacks drawing considerable attention for their high stealth and strong persistence capabilities.",
    "sources": [
      {
        "type": "paper",
        "title": "TrojanRobot: Physical-world Backdoor Attacks Against VLM-based Robotic Manipulation",
        "url": "https://arxiv.org/abs/2411.11683v5"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2411.11683v5"
      },
      {
        "type": "website",
        "title": "Reference",
        "url": "https://trojanrobot.github.io"
      }
    ],
    "tags": [
      "Physical Backdoor",
      "Robotic Manipulation",
      "Vision-Language Models",
      "Module Poisoning"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 20,
    "title": "Exploring the Adversarial Vulnerabilities of Vision-Language-Action Models in Robotics",
    "year": "18 Nov 2024",
    "summary": "Recently in robotics, Vision-Language-Action (VLA) models have emerged as a transformative approach, enabling robots to execute complex tasks by integrating visual and linguistic inputs within an end-to-end learning framework. Despite their significant capabilities, VLA models introduce new attack surfaces.",
    "sources": [
      {
        "type": "paper",
        "title": "Exploring the Adversarial Vulnerabilities of Vision-Language-Action Models in Robotics",
        "url": "https://arxiv.org/abs/2411.13587v4"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2411.13587v4"
      },
      {
        "type": "code",
        "title": "Source code",
        "url": "https://github.com/William-wAng618/roboticAttack"
      },
      {
        "type": "website",
        "title": "Reference",
        "url": "https://vlaattacker.github.io/"
      }
    ],
    "tags": [
      "Vision-Language-Action",
      "Adversarial Patch",
      "Robotic Control",
      "Trajectory Manipulation"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 21,
    "title": "Adversarial Attacks on Robotic Vision Language Action Models",
    "year": "03 Jun 2025",
    "summary": "The emergence of vision-language-action models (VLAs) for end-to-end control is reshaping the field of robotics by enabling the fusion of multimodal sensory inputs at the billion-parameter scale. The capabilities of VLAs stem primarily from their architectures, which are often based on frontier large language models (LLMs).",
    "sources": [
      {
        "type": "paper",
        "title": "Adversarial Attacks on Robotic Vision Language Action Models",
        "url": "https://arxiv.org/abs/2506.03350v1"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2506.03350v1"
      },
      {
        "type": "code",
        "title": "Source code",
        "url": "https://github.com/eliotjones1/robogcg"
      }
    ],
    "tags": [
      "Vision-Language-Action",
      "Jailbreaking",
      "Robotic Control Takeover",
      "Prompt-Based Attack"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 22,
    "title": "Jailbreaking LLM-Controlled Robots",
    "year": "17 Oct 2024",
    "summary": "The recent introduction of large language models (LLMs) has revolutionized the field of robotics by enabling contextual reasoning and intuitive human-robot interaction in domains as varied as manipulation, locomotion, and self-driving vehicles. When viewed as a stand-alone technology, LLMs are known to be vulnerable to jailbreaking attacks, wherein malicious prompters elicit harmful text by bypassing LLM safety guardrails.",
    "sources": [
      {
        "type": "paper",
        "title": "Jailbreaking LLM-Controlled Robots",
        "url": "https://arxiv.org/abs/2410.13691v2"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2410.13691v2"
      },
      {
        "type": "website",
        "title": "Reference",
        "url": "https://robopair.org"
      },
      {
        "type": "article",
        "title": "Wired coverage of robo jailbreaks",
        "url": "https://www.wired.com/story/researchers-llm-ai-robot-violence"
      }
    ],
    "tags": [
      "Jailbreaking",
      "Robotics",
      "Safety Vulnerability",
      "Prompt-Based Attack"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 23,
    "title": "CAPAA: Classifier-Agnostic Projector-Based Adversarial Attack",
    "year": "01 Jun 2025",
    "summary": "Projector-based adversarial attack aims to project carefully designed light patterns (i.e., adversarial projections) onto scenes to deceive deep image classifiers. It has potential applications in privacy protection and the development of more robust classifiers.",
    "sources": [
      {
        "type": "paper",
        "title": "CAPAA: Classifier-Agnostic Projector-Based Adversarial Attack",
        "url": "https://arxiv.org/abs/2506.00978v2"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2506.00978v2"
      },
      {
        "type": "code",
        "title": "Source code",
        "url": "https://github.com/ZhanLiQxQ/CAPAA"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Projector Attack",
      "Multi-Classifier",
      "Stealth Perturbation"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 24,
    "title": "Cybersecurity AI: Humanoid Robots as Attack Vectors",
    "year": "17 Sep 2025",
    "summary": "Security researchers document how the Unitree G1 humanoid can be compromised through a BLE provisioning command injection that yields root access, aided by shared AES keys across devices. They further expose weaknesses in the robot's FMX encryption and persistent telemetry exfiltration, demonstrating how an onboard cybersecurity AI can escalate from reconnaissance to active offensive operations against external targets.",
    "sources": [
      {
        "type": "paper",
        "title": "Cybersecurity AI: Humanoid Robots as Attack Vectors",
        "url": "https://arxiv.org/abs/2509.14139v3"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2509.14139v3"
      }
    ],
    "tags": [
      "Humanoid Robot Security",
      "Cybersecurity AI",
      "Robotic Control",
      "Autonomous Systems"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 25,
    "title": "The Cybersecurity of a Humanoid Robot",
    "year": "16 Sep 2025",
    "summary": "Provides a comprehensive penetration test of a production humanoid platform, cataloging firmware, wireless, and actuator vulnerabilities that enable persistent root access and remote control.",
    "sources": [
      {
        "type": "paper",
        "title": "The Cybersecurity of a Humanoid Robot",
        "url": "https://arxiv.org/abs/2509.14096"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2509.14096"
      }
    ],
    "tags": [
      "Humanoid Robot Security",
      "Vulnerability Assessment",
      "BLE Exploitation",
      "Remote Control"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 26,
    "title": "Robot Hacking Manual (RHM)",
    "year": "09 Mar 2022",
    "summary": "Compiles offensive security techniques for industrial and service robots, covering attack surfaces from network middleware to safety controllers with reproducible lab setups.",
    "sources": [
      {
        "type": "paper",
        "title": "Robot Hacking Manual (RHM)",
        "url": "https://arxiv.org/abs/2203.04765"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2203.04765"
      }
    ],
    "tags": [
      "Robotics Security",
      "Offensive Security",
      "Industrial Control",
      "Hands-On Labs"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 27,
    "title": "Offensive Robot Cybersecurity",
    "year": "23 Jun 2025",
    "summary": "Details a red-teaming methodology for robotic platforms that couples adversarial simulation, exploit development, and mitigations tailored to autonomous systems.",
    "sources": [
      {
        "type": "paper",
        "title": "Offensive Robot Cybersecurity",
        "url": "https://arxiv.org/abs/2506.15343"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2506.15343"
      }
    ],
    "tags": [
      "Robotics Security",
      "Red Team",
      "Penetration Testing",
      "Autonomous Systems"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 28,
    "title": "SoK: Cybersecurity Assessment of Humanoid Ecosystem",
    "year": "27 Aug 2025",
    "summary": "Systematizes known vulnerabilities across humanoid robot hardware, software supply chains, and cloud services, prioritizing mitigations for safety-critical deployments.",
    "sources": [
      {
        "type": "paper",
        "title": "SoK: Cybersecurity Assessment of Humanoid Ecosystem",
        "url": "https://arxiv.org/abs/2508.17481"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2508.17481"
      }
    ],
    "tags": [
      "Humanoid Robots",
      "Security Assessment",
      "Supply Chain",
      "Risk Prioritization"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 29,
    "title": "DevSecOps in Robotics",
    "year": "23 Mar 2020",
    "summary": "Advocates for integrating security practices into the robotics software lifecycle, outlining continuous integration, testing, and deployment controls tailored to ROS ecosystems.",
    "sources": [
      {
        "type": "paper",
        "title": "DevSecOps in Robotics",
        "url": "https://arxiv.org/abs/2003.10402"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2003.10402"
      }
    ],
    "tags": [
      "Robotics Security",
      "DevSecOps",
      "Secure Development",
      "Continuous Integration"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 30,
    "title": "Alurity: A Systematic Review on the Security of Robotic and Autonomous Systems",
    "year": "25 Mar 2022",
    "summary": "Surveys research on cyber-physical threats to robots and autonomous platforms, synthesizing mitigation strategies across networking, perception, and control layers.",
    "sources": [
      {
        "type": "paper",
        "title": "Alurity: A Systematic Review on the Security of Robotic and Autonomous Systems",
        "url": "https://arxiv.org/abs/2203.13874"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2203.13874"
      }
    ],
    "tags": [
      "Robotics Security",
      "Systematic Review",
      "Autonomous Systems",
      "Vulnerability Analysis"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 31,
    "title": "Invisible Perturbations: Physical Adversarial Examples Exploiting the Rolling Shutter Effect",
    "year": "26 Nov 2020",
    "summary": "Demonstrates that modulated light sources can exploit rolling shutter readout to imprint invisible stripe patterns on captured images, enabling remote, human-imperceptible misclassification of ImageNet targets by commodity camera pipelines.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2011.13375"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2011.13375"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Rolling Shutter",
      "Optical Attack",
      "Invisible Perturbation"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 32,
    "title": "Rolling Colors: Adversarial Laser Exploits against Traffic Light Recognition",
    "year": "10 Aug 2022",
    "summary": "Analyzes how pulsed, multi-color laser projections can spoof traffic light recognition systems in autonomous vehicles by overloading camera sensors, and studies countermeasures spanning sensing hardware, perception, and control layers.",
    "sources": [
      {
        "type": "paper",
        "title": "USENIX Security 2022 presentation",
        "url": "https://www.usenix.org/conference/usenixsecurity22/presentation/yan"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://www.usenix.org/system/files/sec22-yan.pdf"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Laser Attack",
      "Traffic Infrastructure",
      "Autonomous Driving"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 33,
    "title": "Security Analysis of Camera-LiDAR Fusion Against Black-Box Attacks on Autonomous Vehicles",
    "year": "10 Aug 2022",
    "summary": "Evaluates the resilience of camera-LiDAR fusion stacks against black-box adversaries, presenting real-world spoofing attacks and hardening guidance for perception fusion used by autonomous cars.",
    "sources": [
      {
        "type": "paper",
        "title": "USENIX Security 2022 presentation",
        "url": "https://www.usenix.org/conference/usenixsecurity22/presentation/hallyburton"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://www.usenix.org/system/files/sec22-hallyburton.pdf"
      }
    ],
    "tags": [
      "Sensor Fusion",
      "Autonomous Driving",
      "LiDAR Spoofing",
      "Physical Adversarial"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 34,
    "title": "On the Realism of LiDAR Spoofing Attacks against Autonomous Driving Vehicle at High Speed and Long Distance",
    "year": "24 Feb 2025",
    "summary": "Shows experimentally that frequency-modulated LiDAR spoofing can introduce phantom obstacles against vehicles traveling at highway speeds and ranges, and analyzes mitigation strategies for automotive perception stacks.",
    "sources": [
      {
        "type": "paper",
        "title": "NDSS 2025 paper",
        "url": "https://www.ndss-symposium.org/ndss-paper/on-the-realism-of-lidar-spoofing-attacks-against-autonomous-driving-vehicle-at-high-speed-and-long-distance/"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://www.ndss-symposium.org/wp-content/uploads/2025-628-paper.pdf"
      }
    ],
    "tags": [
      "LiDAR Spoofing",
      "Autonomous Driving",
      "Physical Adversarial",
      "Sensor Security"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 35,
    "title": "Imperceptible CMOS camera dazzle for adversarial attacks on deep neural networks",
    "year": "22 Oct 2023",
    "summary": "Introduces an optical dazzle attack that blinds CMOS rolling-shutter cameras under photopic conditions invisible to humans, causing downstream vision models to fail while keeping the light source covert.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2311.16118"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2311.16118"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Optical Attack",
      "Camera Sensor Spoofing",
      "Stealth"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 36,
    "title": "Shadows can be Dangerous: Stealthy and Effective Physical-world Adversarial Attack by Natural Phenomenon",
    "year": "08 Mar 2022",
    "summary": "Crafts shadow-based perturbations that harness natural illumination to mislead traffic sign recognizers under black-box assumptions, demonstrating high success in simulation and real driving scenarios.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2203.03818"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2203.03818"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Shadow Attack",
      "Traffic Sign Spoofing",
      "Black-Box"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 37,
    "title": "Adversarial Light Projection Attacks on Face Recognition Systems: A Feasibility Study",
    "year": "14 Jun 2020",
    "summary": "Validates projector-based adversarial patterns that reliably misidentify subjects in face recognition systems across physical environments, quantifying success rates under varied lighting and motion.",
    "sources": [
      {
        "type": "paper",
        "title": "CVPR 2020 Workshop paper",
        "url": "https://openaccess.thecvf.com/content_CVPRW_2020/papers/w48/Nguyen_Adversarial_Light_Projection_Attacks_on_Face_Recognition_Systems_A_Feasibility_CVPRW_2020_paper.pdf"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Projector Attack",
      "Face Recognition",
      "Optical Attack"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 38,
    "title": "ProjAttacker: A Configurable Physical Adversarial Attack for Face Recognition via Projector",
    "year": "19 Jun 2025",
    "summary": "Introduces a configurable framework for generating projector-based adversarial patterns that adapt to diverse camera poses and face models, improving stealth and transferability of physical face attacks.",
    "sources": [
      {
        "type": "paper",
        "title": "CVPR 2025 paper",
        "url": "https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_ProjAttacker_A_Configurable_Physical_Adversarial_Attack_for_Face_Recognition_via_CVPR_2025_paper.pdf"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Projector Attack",
      "Face Recognition",
      "Configurable"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 39,
    "title": "Adversarial Attacks on Event-Based Pedestrian Detectors: A Physical Approach",
    "year": "01 Mar 2025",
    "summary": "Designs clothing-borne textures that transfer from simulation to real-world evaluations, degrading event-camera pedestrian detectors and revealing vulnerabilities in neuromorphic perception stacks.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2503.00377"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2503.00377"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Event Cameras",
      "Pedestrian Detection",
      "Adversarial Clothing"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 41,
    "title": "Safeguarding Vision-Language Models Against Patched Visual Prompt Injectors",
    "year": "17 May 2024",
    "summary": "Proposes SmoothVLM, a smoothing-based defense that counters adversarial patch prompt injections on VLMs, reducing attack success while preserving task fidelity across benchmark datasets.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2405.10529"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2405.10529"
      }
    ],
    "tags": [
      "Vision-Language Models",
      "Prompt Injection",
      "Defense",
      "Adversarial Patch"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 43,
    "title": "TrojVLM: Backdoor Attack Against Vision Language Models",
    "year": "28 Sep 2024",
    "summary": "Introduces TrojVLM, a backdoor that injects attacker-controlled text into VLM outputs while preserving semantic fidelity, demonstrating risks for image captioning and VQA pipelines.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2409.19232"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2409.19232"
      }
    ],
    "tags": [
      "Vision-Language Models",
      "Backdoor Attack",
      "Image Captioning",
      "Adversarial NLP"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 44,
    "title": "BadCLIP: Trigger-Aware Prompt Learning for Backdoor Attacks on CLIP",
    "year": "26 Nov 2023",
    "summary": "Develops BadCLIP, a trigger-aware prompt tuning method that implants persistent backdoors into CLIP without extensive data, achieving high attack success while retaining clean accuracy across datasets.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2311.16194"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2311.16194"
      }
    ],
    "tags": [
      "CLIP",
      "Backdoor Attack",
      "Prompt Learning",
      "Vision-Language Models"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 45,
    "title": "Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models",
    "year": "05 Feb 2024",
    "summary": "Presents Shadowcast, a data poisoning pipeline that hides malicious perturbations in VLM training pairs to drive both label flipping and persuasive misinformation behaviors with minimal poison budget.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2402.06659"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2402.06659"
      }
    ],
    "tags": [
      "Vision-Language Models",
      "Data Poisoning",
      "Stealth Attack",
      "Misinformation"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 46,
    "title": "Robust Contrastive Language-Image Pre-training against Data Poisoning and Backdoor Attacks",
    "year": "13 Mar 2023",
    "summary": "Introduces ROCLIP, a robust CLIP training strategy that breaks poisoned image-caption associations via caption pools and augmentations, cutting targeted attack success while retaining downstream accuracy.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2303.06854"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2303.06854"
      }
    ],
    "tags": [
      "CLIP",
      "Defense",
      "Data Poisoning",
      "Vision-Language Models"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 48,
    "title": "Jailbreaking Black Box Large Language Models in Twenty Queries",
    "year": "12 Oct 2023",
    "summary": "Introduces Prompt Automatic Iterative Refinement (PAIR), a black-box jailbreak algorithm that efficiently crafts adversarial prompts for closed-source LLMs with fewer than twenty interactions.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2310.08419"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2310.08419"
      }
    ],
    "tags": [
      "LLM Security",
      "Jailbreak Attack",
      "Prompt Engineering",
      "Black-Box"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 49,
    "title": "TrojanRobot: Physical-world Backdoor Attacks Against VLM-based Robotic Manipulation",
    "year": "18 Nov 2024",
    "summary": "Backdoors modular robotic policies by inserting poisoned VLM perception modules, demonstrating high-success physical attacks across UR3e manipulator tasks with stealthy trigger designs.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2411.11683"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2411.11683"
      }
    ],
    "tags": [
      "Robotics Security",
      "Backdoor Attack",
      "Vision-Language Models",
      "Physical Adversarial"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 50,
    "title": "Attacking Vision-Language Computer Agents via Pop-ups",
    "year": "04 Nov 2024",
    "summary": "Demonstrates that adversarial pop-up windows can reliably derail VLM-driven computer agents, cutting benchmark success rates nearly in half despite simple instruction-level defenses.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2411.02391"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2411.02391"
      }
    ],
    "tags": [
      "Vision-Language Models",
      "Agent Security",
      "Prompt Injection",
      "Human-Computer Interaction"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 51,
    "title": "The Translucent Patch: A Physical and Universal Attack on Object Detectors",
    "year": "23 Dec 2020",
    "summary": "Designs camera-mounted translucent patches that universally hide chosen object classes from state-of-the-art detectors while leaving other detections largely intact in autonomous driving benchmarks.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2012.12528"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2012.12528"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Object Detection",
      "Universal Attack",
      "Autonomous Driving"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 52,
    "title": "Physical Backdoor: Towards Temperature-based Backdoor Attacks in the Physical World",
    "year": "30 Apr 2024",
    "summary": "Introduces thermal infrared triggers that plant backdoors in TIOD models, detailing temperature and material considerations for high-success attacks in both digital and field tests.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2404.19417"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2404.19417"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Backdoor Attack",
      "Thermal Imaging",
      "Autonomous Driving"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 53,
    "title": "Adversarial Robustness for Visual Grounding of Multimodal Large Language Models",
    "year": "16 May 2024",
    "summary": "Analyzes adversarial attacks on multimodal LLM visual grounding, proposing untargeted and targeted perturbations that misalign bounding boxes and benchmarking robustness across state-of-the-art models.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2405.09981"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2405.09981"
      }
    ],
    "tags": [
      "Vision-Language Models",
      "Adversarial Attack",
      "Visual Grounding",
      "Robustness"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 56,
    "title": "Exploring the Adversarial Vulnerabilities of Vision-Language-Action Models in Robotics",
    "year": "18 Nov 2024",
    "summary": "Benchmarks untargeted and targeted attacks plus adversarial patches against VLA robots, revealing drastic drops in task success and highlighting spatially-informed threat objectives.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2411.13587"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2411.13587"
      }
    ],
    "tags": [
      "Robotics Security",
      "Vision-Language Models",
      "Adversarial Attack",
      "VLA"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 57,
    "title": "RFLA: A Stealthy Reflected Light Adversarial Attack in the Physical World",
    "year": "14 Jul 2023",
    "summary": "Uses reflected colored light patterns to craft stealthy adversarial perturbations effective under daylight, validated across datasets and physical setups without conspicuous markers.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2307.07653"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2307.07653"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Optical Attack",
      "Stealth",
      "Reflected Light"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 58,
    "title": "Invisible Optical Adversarial Stripes on Traffic Sign against Autonomous Vehicles",
    "year": "10 Jul 2024",
    "summary": "Builds GhostStripe, a timing-controlled LED attack that creates invisible rolling-shutter stripes on traffic signs, achieving sustained misclassification of autonomous vehicle perception.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2407.07510"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2407.07510"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Rolling Shutter",
      "Traffic Sign Spoofing",
      "Autonomous Driving"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 59,
    "title": "Adversarial Laser Spot: Robust and Covert Physical-World Attack to DNNs",
    "year": "02 Jun 2022",
    "summary": "Optimizes low-cost laser spot parameters via genetic algorithms to conduct daytime-visible yet covert attacks that transfer across deep vision models.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2206.01034"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2206.01034"
      },
      {
        "type": "code",
        "title": "Source code",
        "url": "https://github.com/ChengYinHu/AdvLS"
      }
    ],
    "tags": [
      "Physical Adversarial",
      "Laser Attack",
      "Robustness",
      "Transferability"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 60,
    "title": "State-of-the-art optical-based physical adversarial attacks for deep learning computer vision systems",
    "year": "22 Mar 2023",
    "summary": "Surveys invasive and non-invasive optical physical attacks, categorizing techniques such as lasers, projectors, and reflective perturbations against deep vision models.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2303.12249"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2303.12249"
      }
    ],
    "tags": [
      "Survey",
      "Physical Adversarial",
      "Optical Attack",
      "Computer Vision"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 61,
    "title": "Phantom of the ADAS: Securing Advanced Driver-Assistance Systems from Split-Second Phantom Attacks",
    "year": "09 Nov 2020",
    "summary": "Analyzes projection-based phantom imagery that tricks ADAS perception within milliseconds, and proposes hardware-software defenses tailored to production autonomous driving stacks.",
    "sources": [
      {
        "type": "paper",
        "title": "ACM CCS 2020",
        "url": "https://dl.acm.org/doi/10.1145/3372297.3423359"
      }
    ],
    "tags": [
      "Autonomous Driving",
      "Sensor Spoofing",
      "Security Assessment",
      "Physical Adversarial"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 62,
    "title": "On the Vulnerability of LLM/VLM-Controlled Robotics",
    "year": "15 Feb 2024",
    "summary": "Characterizes how small instruction and perception perturbations degrade LLM/VLM robot controllers, offering perturbation strategies that expose 14\u201322% drops in task success.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2402.10340"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2402.10340"
      }
    ],
    "tags": [
      "Robotics Security",
      "LLM Vulnerability",
      "Vision-Language Models",
      "Robustness"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 64,
    "title": "Latent Diffusion Models are Powerful Visual Anomaly Detectors",
    "year": "22 Jul 2022",
    "summary": "Demonstrates that latent diffusion models pretrained on generic imagery provide rich feature spaces for anomaly detection, achieving state-of-the-art accuracy on MVTec AD and VisA benchmarks without task-specific supervision.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2207.11569"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2207.11569"
      }
    ],
    "tags": [
      "Anomaly Detection",
      "Latent Diffusion",
      "Industrial Inspection",
      "Computer Vision"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 65,
    "title": "Evaluating Uncertainty and Quality of Visual Language Action-enabled Robots",
    "year": "22 Jul 2025",
    "summary": "Introduces eight uncertainty and five quality metrics tailored for VLA robot controllers, showing through 908 manipulation trials that several measures align with expert judgments and distinguish low-quality executions beyond simple success rates.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2507.17049"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2507.17049"
      }
    ],
    "tags": [
      "Vision-Language Action",
      "Uncertainty Estimation",
      "Robotics Evaluation",
      "Task Quality"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 66,
    "title": "SafeAgentBench: A Benchmark for Safe Task Planning of Embodied LLM Agents",
    "year": "17 Dec 2024",
    "summary": "Introduces SafeAgentBench, a benchmark of 750 tasks and a universal embodied environment to evaluate safety-aware planning by embodied LLM agents across hazardous scenarios. Reports that state-of-the-art agents succeed on safe tasks yet rarely reject hazardous ones, underscoring persistent safety gaps.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2412.13178"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2412.13178"
      },
      {
        "type": "code",
        "title": "Source code",
        "url": "https://github.com/shengyin1224/SafeAgentBench"
      }
    ],
    "tags": [
      "Safety Benchmark",
      "Embodied Agents",
      "Vision-Language Models",
      "Risk Assessment"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 67,
    "title": "RiskAwareBench: Towards Evaluating Physical Risk Awareness for High-level Planning of LLM-based Embodied Agents",
    "year": "08 Aug 2024",
    "summary": "Presents RiskAwareBench, an automated framework for generating risky scenes, safety tips, and evaluation protocols that measure physical risk awareness in LLM-based embodied planners. Finds that current language models show limited hazard recognition and that baseline mitigation strategies offer minimal improvement.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2408.04449"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2408.04449"
      }
    ],
    "tags": [
      "Safety Benchmark",
      "Risk Awareness",
      "Embodied Agents",
      "Vision-Language Models"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 68,
    "title": "BadRobot: Jailbreaking Embodied LLMs in the Physical World",
    "year": "16 Jul 2024",
    "summary": "Demonstrates BadRobot, a voice-driven jailbreak that exploits planning misalignments in embodied LLM frameworks to elicit hazardous physical actions. Constructs a benchmark of malicious queries and shows successful attacks against systems like Voxposer, Code as Policies, and ProgPrompt.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2407.20242"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2407.20242"
      }
    ],
    "tags": [
      "LLM Jailbreak",
      "Embodied Agents",
      "Security Assessment",
      "Adversarial Attack"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 69,
    "title": "Generating Robot Constitutions & Benchmarks for Semantic Safety",
    "year": "11 Mar 2025",
    "summary": "Introduces the ASIMOV Benchmark to stress-test semantic safety in foundation-model robot controllers and presents an auto-amending pipeline that derives robot constitutions from real-world injury data, boosting rejection of unsafe actions via Constitutional AI.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2503.08663v1"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2503.08663v1"
      },
      {
        "type": "website",
        "title": "ASIMOV Benchmark",
        "url": "https://asimov-benchmark.github.io"
      }
    ],
    "tags": [
      "Semantic Safety",
      "Constitutional AI",
      "Benchmark",
      "Foundation Models"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 70,
    "title": "HAZARD Challenge: Embodied Decision Making in Dynamically Changing Environments",
    "year": "23 Jan 2024",
    "summary": "Presents the HAZARD benchmark of disaster-themed virtual scenarios that stress-test how embodied agents using reinforcement learning, search, or LLM-based planning adapt to rapidly changing environments and unexpected hazards.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2401.12975"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2401.12975"
      },
      {
        "type": "website",
        "title": "Project page",
        "url": "https://vis-www.cs.umass.edu/hazard/"
      }
    ],
    "tags": [
      "Safety Benchmark",
      "Embodied Agents",
      "Dynamic Environments",
      "Disaster Response"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 71,
    "title": "Red Teaming Visual Language Models",
    "year": "23 Jan 2024",
    "summary": "Introduces an automated red teaming pipeline that composes adversarial multimodal prompts to probe visual language models, revealing systematic safety gaps even after instruction tuning and mitigation heuristics.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2401.12915"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2401.12915v1"
      }
    ],
    "tags": [
      "Red Teaming",
      "Vision-Language Models",
      "Safety Evaluation",
      "Adversarial Prompting"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 72,
    "title": "Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large   Language Models",
    "year": "03 Feb 2024",
    "summary": "Demonstrates that lightweight LoRA safety fine-tuning with curated refusal and harmless response data can substantially reduce jailbreak success rates for vision-language models with negligible utility loss.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2402.02207"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2402.02207v2"
      }
    ],
    "tags": [
      "Safety Fine-Tuning",
      "Vision-Language Models",
      "LoRA",
      "Alignment"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 73,
    "title": "Safe Inputs but Unsafe Output: Benchmarking Cross-modality Safety   Alignment of Large Vision-Language Model",
    "year": "21 Jun 2024",
    "summary": "Proposes a benchmark for cross-modality safety alignment that pairs benign images with risky text instructions, showing that leading large vision-language models still emit unsafe outputs despite safe inputs.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2406.15279"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2406.15279v2"
      }
    ],
    "tags": [
      "Safety Benchmark",
      "Cross-Modality",
      "Vision-Language Models",
      "Risk Assessment"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 74,
    "title": "MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large   Language Models",
    "year": "29 Nov 2023",
    "summary": "Builds MM-SafetyBench with diverse multimodal safety tasks and evaluations, highlighting inconsistent refusals and hallucinated harmful guidance across state-of-the-art multimodal language models.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2311.17600"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2311.17600v5"
      }
    ],
    "tags": [
      "Safety Benchmark",
      "Multimodal LLMs",
      "Risk Assessment",
      "Dataset"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 75,
    "title": "SafeBench: A Safety Evaluation Framework for Multimodal Large Language   Models",
    "year": "24 Oct 2024",
    "summary": "Introduces SafeBench, an extensible safety evaluation framework that unifies risk taxonomies, scenario generation, and automatic scoring to audit multimodal large language models across threat categories.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2410.18927"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2410.18927v1"
      }
    ],
    "tags": [
      "Safety Evaluation",
      "Multimodal LLMs",
      "Benchmark",
      "Risk Assessment"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 76,
    "title": "MultiTrust: A Comprehensive Benchmark Towards Trustworthy Multimodal   Large Language Models",
    "year": "11 Jun 2024",
    "summary": "Releases MultiTrust, a benchmark spanning safety, robustness, and fairness dimensions to gauge the overall trustworthiness of multimodal large language models and expose remaining vulnerabilities.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2406.07057"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2406.07057v2"
      }
    ],
    "tags": [
      "Trustworthiness",
      "Multimodal LLMs",
      "Benchmark",
      "Safety Evaluation"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 77,
    "title": "JailBreakV: A Benchmark for Assessing the Robustness of MultiModal Large   Language Models against Jailbreak Attacks",
    "year": "03 Apr 2024",
    "summary": "Presents JailBreakV, a systematic evaluation suite of multimodal jailbreak prompts and defenses, demonstrating how malicious visual-text combinations can bypass existing safety guardrails.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2404.03027"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2404.03027v4"
      }
    ],
    "tags": [
      "Jailbreak Attacks",
      "Multimodal LLMs",
      "Security Benchmark",
      "Adversarial Prompting"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 78,
    "title": "Arondight: Red Teaming Large Vision Language Models with Auto-generated   Multi-modal Jailbreak Prompts",
    "year": "21 Jul 2024",
    "summary": "Develops Arondight, an auto-generated multimodal jailbreak prompt engine that red teams large vision-language models and uncovers failure patterns driving improved defensive training.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2407.15050"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2407.15050v1"
      }
    ],
    "tags": [
      "Red Teaming",
      "Vision-Language Models",
      "Jailbreak Attacks",
      "Automation"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 79,
    "title": "Embodied Red Teaming for Auditing Robotic Foundation Models",
    "year": "27 Nov 2024",
    "summary": "Explores embodied red teaming workflows that script hazardous tasks for robotic foundation models, surfacing unsafe action plans in simulation and real robot executions despite natural-language safety prompts.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2411.18676"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2411.18676v2"
      }
    ],
    "tags": [
      "Embodied Agents",
      "Red Teaming",
      "Robotic Foundation Models",
      "Safety Evaluation"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 80,
    "title": "SPA-VL: A Comprehensive Safety Preference Alignment Dataset for Vision   Language Model",
    "year": "17 Jun 2024",
    "summary": "Collects SPA-VL, a large-scale safety preference alignment dataset for vision-language models, enabling preference optimization that strengthens refusal behavior without degrading core visual reasoning skills.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2406.12030"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2406.12030v4"
      }
    ],
    "tags": [
      "Preference Alignment",
      "Vision-Language Models",
      "Safety Dataset",
      "Alignment"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 81,
    "title": "Adversarial Attacks in Multimodal Systems: A Practitioner's Survey",
    "year": "06 May 2025",
    "summary": "Surveys adversarial threats against multimodal AI, organizing text, image, video, and audio attack techniques and evolutions to guide practitioners on assessing cross-modal vulnerabilities.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2505.03084"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2505.03084"
      }
    ],
    "tags": [
      "Multimodal Models",
      "Adversarial Attacks",
      "Survey",
      "Security"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 82,
    "title": "Inducing Dyslexia in Vision Language Models",
    "year": "29 Sep 2025",
    "summary": "Simulates dyslexia in vision-language models by ablating visual-word-form-selective units, reproducing human-like reading impairments while preserving broader multimodal comprehension.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2509.24597"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2509.24597"
      }
    ],
    "tags": [
      "Vision-Language Models",
      "Neuroscience",
      "Reading Disorders",
      "Model Ablation"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 83,
    "title": "LMMs-Eval: Reality Check on the Evaluation of Large Multimodal Models",
    "year": "17 Jul 2024",
    "summary": "Introduces LMMS-EVAL, a standardized benchmark spanning 50+ multimodal tasks and 10+ models, and complements it with LMMS-EVAL LITE and LiveBench to balance coverage, cost, and contamination in LMM evaluation.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2407.12772"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "http://arxiv.org/pdf/2407.12772"
      }
    ],
    "tags": [
      "Vision-Language Models",
      "Benchmark",
      "Evaluation",
      "Multimodal"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 84,
    "title": "BEHAVIOR Benchmark and Challenge",
    "year": "09 Oct 2025",
    "summary": "Official site for Stanford's BEHAVIOR benchmark, aggregating BEHAVIOR-1K's long-horizon household task dataset, OmniGibson simulator resources, documentation, and the 2025 challenge hub for embodied AI research.",
    "sources": [
      {
        "type": "website",
        "title": "BEHAVIOR",
        "url": "https://behavior.stanford.edu"
      }
    ],
    "tags": [
      "Embodied AI",
      "Household Tasks",
      "Simulation Benchmark",
      "Robotic Manipulation"
    ],
    "last_reviewed": "09 Oct 2025"
  },
  {
    "id": 85,
    "title": "Hidden in plain sight: VLMs overlook their visual representations",
    "year": "09 Jun 2025",
    "summary": "Shows that open-source vision-language models underperform their visual encoders on vision-centric tasks, tracing failures to the language component that neglects accessible visual information and inherits strong language priors.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2506.08008"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2506.08008"
      }
    ],
    "tags": [
      "Vision-Language Models",
      "Representation Analysis",
      "Benchmark",
      "Evaluation"
    ],
    "last_reviewed": "09 Jun 2025"
  },
  {
    "id": 86,
    "title": "State of Vision-Language-Action (VLA) Research at ICLR 2026",
    "year": "10 Oct 2025",
    "summary": "Synthesizes 164 ICLR 2026 VLA submissions, spotlighting discrete diffusion agents, embodied reasoning pipelines, tokenizer innovations, efficiency pushes, and evaluation work while explaining how to interpret benchmark reports and the remaining gap between frontier and academic labs.",
    "sources": [
      {
        "type": "article",
        "title": "Moritz Reuss Blog",
        "url": "https://mbreuss.github.io/blog_post_iclr_26_vla.html"
      }
    ],
    "tags": [
      "Vision-Language-Action",
      "Survey",
      "Benchmark",
      "Embodied AI"
    ],
    "last_reviewed": "10 Oct 2025"
  },
  {
    "id": 87,
    "title": "SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Constrained Learning",
    "year": "05 Mar 2025",
    "summary": "Introduces SafeVLA, an integrated safety alignment pipeline that elicits unsafe behaviors, optimizes VLAs with constrained reinforcement learning, and validates them through targeted evaluations to deliver large safety gains without sacrificing task performance.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2503.03480"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2503.03480"
      }
    ],
    "tags": [
      "Vision-Language-Action",
      "Safety",
      "Reinforcement Learning",
      "Robotics"
    ],
    "last_reviewed": "05 Mar 2025"
  },
  {
    "id": 88,
    "title": "Humanoid Everyday: A Comprehensive Robotic Dataset for Open-World Humanoid Manipulation",
    "year": "09 Oct 2025",
    "summary": "Releases Humanoid Everyday, a 10.3k-trajectory multimodal humanoid manipulation dataset spanning 260 tasks across seven categories, collected via human-supervised teleoperation, and pairs it with policy benchmarks plus a cloud platform for standardized evaluation.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2510.08807"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2510.08807"
      }
    ],
    "tags": [
      "Humanoid Robotics",
      "Dataset",
      "Multimodal",
      "Teleoperation"
    ],
    "last_reviewed": "09 Oct 2025"
  },
  {
    "id": 89,
    "title": "Robot Learning: A Tutorial",
    "year": "14 Oct 2025",
    "summary": "Guides researchers through the modern robot learning landscape, connecting foundational reinforcement and imitation learning to generalist, language-conditioned policies and providing practical tooling via the Hugging Face LeRobot library.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2510.12403"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2510.12403"
      }
    ],
    "tags": [
      "Robot Learning",
      "Tutorial",
      "Reinforcement Learning",
      "Imitation Learning"
    ],
    "last_reviewed": "14 Oct 2025"
  },
  {
    "id": 90,
    "title": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey",
    "year": "23 Sep 2025",
    "summary": "Surveys over three hundred pure VLA models, organizing them into autoregressive, diffusion, reinforcement, hybrid, and specialized paradigms while detailing datasets, benchmarks, and open challenges for generalizable robotic agents.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2509.19012"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2509.19012"
      }
    ],
    "tags": [
      "Vision-Language-Action",
      "Survey",
      "Benchmark",
      "Robotics"
    ],
    "last_reviewed": "23 Sep 2025"
  },
  {
    "id": 91,
    "title": "FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models",
    "year": "24 Sep 2025",
    "summary": "Introduces FreezeVLA, a min-max optimized adversarial image attack that disconnects VLA policies from their actions, freezing robots across three models and four benchmarks with 76.2% average success and highlighting urgent safety risks.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2509.19870"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2509.19870"
      }
    ],
    "tags": [
      "Vision-Language-Action",
      "Adversarial Attack",
      "Robotic Control",
      "Safety"
    ],
    "last_reviewed": "24 Sep 2025"
  },
  {
    "id": 92,
    "title": "Model-agnostic Adversarial Attack and Defense for Vision-Language-Action Models",
    "year": "15 Oct 2025",
    "summary": "Proposes the model-agnostic Embedding Disruption Patch Attack that breaks visual-text alignment in VLAs and a robustness-oriented encoder fine-tuning defense, showing large performance drops and recovery on LIBERO benchmarks.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2510.13237"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2510.13237"
      },
      {
        "type": "website",
        "title": "Project page",
        "url": "https://edpa-attack.github.io/"
      }
    ],
    "tags": [
      "Vision-Language-Action",
      "Adversarial Patch",
      "Robustness",
      "Defense"
    ],
    "last_reviewed": "15 Oct 2025"
  },
  {
    "id": 93,
    "title": "Goal-oriented Backdoor Attack against Vision-Language-Action Models via Physical Objects",
    "year": "10 Oct 2025",
    "summary": "Demonstrates GoBA, a physical trigger backdoor pipeline that plants goal-conditioned behaviors into VLAs through dataset poisoning, achieving 97% backdoor success on LIBERO while maintaining clean-task performance and introducing the BadLIBERO benchmark.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2510.09269"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2510.09269"
      },
      {
        "type": "website",
        "title": "Project page",
        "url": "https://goba-attack.github.io/"
      }
    ],
    "tags": [
      "Vision-Language-Action",
      "Backdoor Attack",
      "Physical Adversarial",
      "Robotic Manipulation"
    ],
    "last_reviewed": "10 Oct 2025"
  },
  {
    "id": 94,
    "title": "TabVLA: Targeted Backdoor Attacks on Vision-Language-Action Models",
    "year": "13 Oct 2025",
    "summary": "Introduces TabVLA, a black-box targeted backdoor framework that poisons VLA vision streams under realistic inference-time threat models, revealing minimal data requirements, robustness to trigger designs, and partial defenses via latent trigger detection.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2510.10932"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2510.10932"
      }
    ],
    "tags": [
      "Vision-Language-Action",
      "Backdoor Attack",
      "Targeted Attack",
      "Safety"
    ],
    "last_reviewed": "13 Oct 2025"
  },
  {
    "id": 95,
    "title": "Eva-VLA: Evaluating Vision-Language-Action Models' Robustness Under Real-World Physical Variations",
    "year": "23 Sep 2025",
    "summary": "Presents the Eva-VLA framework that casts object transformations, lighting shifts, and adversarial patches as continuous search problems, uncovering over 60% failure rates for OpenVLA agents and exposing deployment gaps under physical variability.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2509.18953"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2509.18953"
      }
    ],
    "tags": [
      "Vision-Language-Action",
      "Robustness",
      "Benchmark",
      "Physical Adversarial"
    ],
    "last_reviewed": "23 Sep 2025"
  },
  {
    "id": 96,
    "title": "LLM-Driven Robots Risk Enacting Discrimination, Violence, and Unlawful Actions",
    "year": "16 Oct 2025",
    "summary": "Benchmarks state-of-the-art language and vision-language models on human-robot interaction tasks covering discrimination, violence, and unlawful conduct, finding widespread unsafe behaviors and highlighting the need for structured risk assessments before deployment.",
    "sources": [
      {
        "type": "paper",
        "title": "SpringerLink",
        "url": "https://link.springer.com/article/10.1007/s12369-025-01301-x"
      },
      {
        "type": "code",
        "title": "GitHub",
        "url": "https://github.com/rumaisa-azeem/llm-robots-discrimination-safety"
      }
    ],
    "tags": [
      "Safety",
      "Safety Evaluation",
      "Risk Assessment",
      "Human-Computer Interaction"
    ],
    "last_reviewed": "16 Oct 2025"
  },
  {
    "id": 97,
    "title": "From Demonstrations to Safe Deployment: Path-Consistent Safety Filtering for Diffusion Policies",
    "year": "09 Nov 2025",
    "summary": "Introduces PACS, a path-consistent safety filter that performs braking along diffusion policy trajectories to preserve demonstration-consistent actions while verifying safety with set-based reachability, enabling real-time deployment that outperforms reactive baselines on dynamic human-robot interaction tasks.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2511.06385"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2511.06385"
      },
      {
        "type": "website",
        "title": "Project page",
        "url": "https://tum-lsy.github.io/pacs/"
      }
    ],
    "tags": [
      "Safety",
      "Diffusion Policy",
      "Robotic Manipulation",
      "Human-Robot Interaction"
    ],
    "last_reviewed": "09 Nov 2025"
  },
  {
    "id": 98,
    "title": "Don't Blind Your VLA: Aligning Visual Representations for OOD Generalization",
    "year": "29 Oct 2025",
    "summary": "Diagnoses how action fine-tuning erodes inherited visual-language representations in VLA models, introduces probing benchmarks that contrast VL and VLA behaviors, and proposes a lightweight alignment method that restores representation quality to improve out-of-distribution generalization.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2510.25616"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2510.25616"
      },
      {
        "type": "website",
        "title": "Project page",
        "url": "https://blind-vla-paper.github.io"
      }
    ],
    "tags": [
      "Vision-Language-Action",
      "Representation Learning",
      "Out-of-Distribution Generalization",
      "Transfer Learning"
    ],
    "last_reviewed": "30 Oct 2025"
  },
  {
    "id": 99,
    "title": "A Study on Enhancing the Generalization Ability of Visuomotor Policies via Data Augmentation",
    "year": "13 Nov 2025",
    "summary": "Automates large-scale trajectory augmentation across manipulators, grippers, and scene layouts to identify which randomization factors most improve visuomotor policy generalization. Demonstrates that diverse visual and physical randomizations significantly boost zero-shot sim-to-real success for low-cost robotic arms.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2511.09932"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2511.09932"
      }
    ],
    "tags": [
      "Robotic Manipulation",
      "Imitation Learning",
      "Data Augmentation",
      "Sim-to-Real Transfer"
    ],
    "last_reviewed": "13 Nov 2025"
  },
  {
    "id": 100,
    "title": "A Study on Prompt Injection Attack Against LLM-Integrated Mobile Robotic Systems",
    "year": "07 Aug 2024",
    "summary": "Analyzes how multimodal prompt injection undermines LLM-integrated mobile robots and evaluates defensive prompt strategies that boost attack detection and navigation reliability by about 30% in mission-oriented tasks.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2408.03515"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2408.03515"
      }
    ],
    "tags": [
      "Prompt Injection",
      "Robotics Security",
      "LLM Safety",
      "Navigation"
    ],
    "last_reviewed": "07 Aug 2024"
  },
  {
    "id": 101,
    "title": "SimWorld: An Open-ended Realistic Simulator for Autonomous Agents in Physical and Social Worlds",
    "year": "01 Dec 2025",
    "summary": "Introduces SimWorld, an Unreal Engine 5-based simulator that procedurally generates rich physical and social environments for training and evaluating LLM/VLM agents with multimodal observations, language-driven scene editing, and realistic physics.",
    "sources": [
      {
        "type": "paper",
        "title": "White paper",
        "url": "https://simworld.org/assets/white_paper.pdf"
      },
      {
        "type": "website",
        "title": "Project site",
        "url": "https://simworld.org"
      },
      {
        "type": "code",
        "title": "Source code",
        "url": "https://github.com/SimWorld-AI/SimWorld"
      }
    ],
    "tags": [
      "Simulation",
      "Embodied AI",
      "LLM Agents",
      "Vision-Language Models",
      "Unreal Engine"
    ],
    "last_reviewed": "02 Dec 2025"
  },
  {
    "id": 102,
    "title": "Opening the Sim-to-Real Door for Humanoid Pixel-to-Action Policy Transfer",
    "year": "30 Nov 2025",
    "summary": "Introduces DoorMan, a teacher-student bootstrap framework that combines staged-reset exploration and GRPO fine-tuning to train an RGB-only humanoid loco-manipulation policy entirely in simulation, enabling zero-shot door opening across diverse real-world doors and outperforming human teleoperators in completion time.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2512.01061"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.01061"
      },
      {
        "type": "website",
        "title": "Project page",
        "url": "https://doorman-humanoid.github.io"
      }
    ],
    "tags": [
      "Humanoid Robotics",
      "Sim-to-Real Transfer",
      "Reinforcement Learning",
      "Loco-Manipulation",
      "Vision-Based Control"
    ],
    "last_reviewed": "02 Dec 2025"
  },
  {
    "id": 103,
    "title": "X-VLA: Soft-Prompted Transformer as Scalable Cross-Embodiment Vision-Language-Action Model",
    "year": "11 Oct 2025",
    "summary": "Presents X-VLA, a flow-matching VLA architecture that injects embodiment-specific soft prompts into standard Transformer encoders to unify training across heterogeneous robot datasets. Demonstrates state-of-the-art generalization across six simulation platforms and three real-world robots, spanning dexterous manipulation and rapid cross-embodiment adaptation.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/pdf/2510.10274"
      },
      {
        "type": "website",
        "title": "Project page",
        "url": "https://thu-air-dream.github.io/X-VLA/"
      },
      {
        "type": "code",
        "title": "Source code",
        "url": "https://github.com/2toinf/X-VLA"
      }
    ],
    "tags": [
      "Vision-Language-Action",
      "Embodied AI",
      "Cross-Embodiment",
      "Soft Prompt",
      "Flow Matching"
    ],
    "last_reviewed": "04 Dec 2025"
  },
  {
    "id": 104,
    "title": "VITRA: Scalable Vision-Language-Action Model Pretraining for Robotic Manipulation with Real-Life Human Activity Videos",
    "year": "24 Oct 2025",
    "summary": "Introduces VITRA, a pipeline that converts large-scale egocentric human hand videos into fully aligned VLA training data with automatically generated action segments, language descriptions, and 3D motion. Pretrains a dexterous-hand VLA model on the resulting 1M-episode dataset, enabling strong zero-shot generalization and improved real-robot manipulation after fine-tuning on limited trajectories.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2510.21571"
      },
      {
        "type": "pdf",
        "title": "Paper",
        "url": "https://microsoft.github.io/VITRA/vitra_paper.pdf"
      },
      {
        "type": "website",
        "title": "Project page",
        "url": "https://microsoft.github.io/VITRA"
      },
      {
        "type": "code",
        "title": "Source code",
        "url": "https://github.com/microsoft/VITRA"
      },
      {
        "type": "model",
        "title": "Model",
        "url": "https://huggingface.co/VITRA-VLA/VITRA-VLA-3B"
      },
      {
        "type": "dataset",
        "title": "VITRA-1M dataset",
        "url": "https://huggingface.co/datasets/VITRA-VLA/VITRA-1M"
      }
    ],
    "tags": [
      "Vision-Language-Action",
      "Dexterous Manipulation",
      "Egocentric Video",
      "Pretraining",
      "Dataset"
    ],
    "last_reviewed": "15 Dec 2025"
  },
  {
    "id": 105,
    "title": "Emergence of Human to Robot Transfer in Vision-Language-Action Models",
    "year": "16 Dec 2025",
    "summary": "Investigates how scaling vision-language-action foundation models unlocks transfer from human activity videos to robotic manipulation, showing that larger models can interpret human demonstrations and execute corresponding robot actions with minimal fine-tuning.",
    "sources": [
      {
        "type": "website",
        "title": "Project page",
        "url": "https://www.pi.website/research/human_to_robot"
      }
    ],
    "tags": [
      "Human-to-Robot Transfer",
      "Vision-Language-Action",
      "Foundation Models",
      "Scaling Laws"
    ],
    "last_reviewed": "16 Dec 2025"
  },
  {
    "id": 106,
    "title": "Moravec's Paradox and the Robot Olympics",
    "year": "22 Dec 2025",
    "summary": "Physical Intelligence fine-tuned its latest vision-language-action model to tackle Benjie Holson's Robot Olympics manipulation challenges, demonstrating broad generalist capabilities and highlighting the gap between cognitive and physical intelligence captured by Moravec's paradox.",
    "sources": [
      {
        "type": "blog",
        "title": "Moravec's Paradox and the Robot Olympics",
        "url": "https://www.pi.website/blog/olympics"
      }
    ],
    "tags": [
      "Robot Olympics",
      "Vision-Language-Action",
      "Robotic Manipulation",
      "Fine-Tuning"
    ],
    "last_reviewed": "22 Dec 2025"
  },
  {
    "id": 107,
    "title": "Robotic VLA Benefits from Joint Learning with Motion Image Diffusion",
    "year": "19 Dec 2025",
    "summary": "Proposes a joint learning strategy that augments vision-language-action models with a diffusion-based motion head to predict optical-flow motion images, improving motion reasoning while keeping standard VLA inference latency and boosting success on LIBERO and RoboTwin benchmarks.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2512.18007"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.18007"
      }
    ],
    "tags": [
      "Vision-Language-Action",
      "Robotic Manipulation",
      "Motion Reasoning",
      "Diffusion"
    ],
    "last_reviewed": "19 Dec 2025"
  },
  {
    "id": 108,
    "title": "VLM4VLA: Revisiting Vision-Language Models in Vision-Language-Action Models",
    "year": "06 Jan 2026",
    "summary": "Introduces VLM4VLA, a lightweight adaptation pipeline that converts general-purpose vision-language models into VLA policies with a small set of learnable parameters, enabling controlled comparisons of VLM choices while remaining competitive with heavier architectures.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2601.03309"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2601.03309"
      },
      {
        "type": "website",
        "title": "Project page",
        "url": "https://cladernyjorn.github.io/VLM4VLA.github.io/"
      }
    ],
    "tags": [
      "Vision-Language-Action",
      "Vision-Language Models",
      "Robotic Manipulation",
      "Policy Learning"
    ],
    "last_reviewed": "06 Jan 2026"
  },
  {
    "id": 109,
    "title": "Video Generation Models in Robotics - Applications, Research Challenges, Future Directions",
    "year": "12 Jan 2026",
    "summary": "Surveys video generation models as embodied world models for robotics, covering applications in data generation, imitation learning, dynamics and reward modeling for reinforcement learning, visual planning, and policy evaluation, while outlining challenges such as instruction following gaps, physics violations, and safety risks.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2601.07823"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2601.07823"
      }
    ],
    "tags": [
      "Survey",
      "Video Generation",
      "World Models",
      "Robotics"
    ],
    "last_reviewed": "12 Jan 2026"
  },
  {
    "id": 110,
    "title": "When Robots Obey the Patch: Universal Transferable Patch Attacks on Vision-Language-Action Models",
    "year": "26 Nov 2025",
    "summary": "Studies universal, transferable adversarial patch attacks against vision-language-action models, showing patches that generalize across architectures and settings and highlighting black-box safety risks for robotic policies.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2511.21192"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2511.21192"
      }
    ],
    "tags": [
      "Vision-Language-Action",
      "Adversarial Patch",
      "Transferable Attack",
      "Robotic Control"
    ],
    "last_reviewed": "26 Nov 2025"
  },
  {
    "id": 111,
    "title": "ADVEDM: Fine-grained Adversarial Attack against VLM-based Embodied Agents",
    "year": "20 Sep 2025",
    "summary": "Introduces ADVEDM, a fine-grained adversarial attack that perturbs visual inputs for VLM-based embodied decision-making agents, degrading planning and action execution in embodied tasks.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2509.16645"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2509.16645"
      }
    ],
    "tags": [
      "Vision-Language Models",
      "Embodied Agents",
      "Adversarial Attack",
      "Robotics"
    ],
    "last_reviewed": "20 Sep 2025"
  },
  {
    "id": 112,
    "title": "mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs",
    "year": "17 Dec 2025",
    "summary": "Introduces mimic-video, a video-action model that pairs an internet-scale pretrained video backbone with a flow-matching action decoder to ground robotic control in visual dynamics, improving sample efficiency and convergence on simulated and real-world manipulation tasks compared to standard VLAs.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2512.15692"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2512.15692"
      },
      {
        "type": "website",
        "title": "Project page",
        "url": "https://mimic-video.github.io/"
      }
    ],
    "tags": [
      "Vision-Language-Action",
      "Video-Action Models",
      "Robotic Manipulation",
      "Imitation Learning"
    ],
    "last_reviewed": "17 Dec 2025"
  },
  {
    "id": 113,
    "title": "ActiveVLA: Injecting Active Perception into Vision-Language-Action Models for Precise 3D Robotic Manipulation",
    "year": "13 Jan 2026",
    "summary": "ActiveVLA injects active perception into vision-language-action models to enable precise 3D robotic manipulation under occlusions and complex spatial structures.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2601.08325v1"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2601.08325v1"
      },
      {
        "type": "website",
        "title": "Project page",
        "url": "https://zhenyangliu.github.io/ActiveVLA/"
      }
    ],
    "tags": [
      "Vision-Language-Action",
      "Active Perception",
      "3D Vision",
      "Robotic Manipulation"
    ],
    "last_reviewed": "17 Jan 2026"
  },
  {
    "id": 114,
    "title": "VLAgents: A Policy Server for Efficient VLA Inference",
    "year": "16 Jan 2026",
    "summary": "Introduces VLAgents, a modular policy server that exposes vision-language-action inference through a unified Gymnasium-style protocol and reduces deployment complexity and communication latency in distributed robotics setups.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2601.11250"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2601.11250"
      }
    ],
    "tags": [
      "Vision-Language-Action",
      "Inference",
      "Robotic Deployment",
      "Systems"
    ],
    "last_reviewed": "16 Jan 2026"
  },
  {
    "id": 115,
    "title": "Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning",
    "year": "22 Jan 2026",
    "summary": "Introduces Cosmos Policy, a fine-tuning method that adapts pretrained video generation models into visuomotor policies without multi-stage post-training or additional action modules, enabling control and planning that leverage spatiotemporal priors from video models.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2601.16163"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2601.16163"
      }
    ],
    "tags": [
      "Video Generation",
      "Visuomotor Control",
      "Planning",
      "Robotic Manipulation"
    ],
    "last_reviewed": "22 Jan 2026"
  },
  {
    "id": 116,
    "title": "Introducing Helix 02: Full-Body Autonomy",
    "year": "27 Jan 2026",
    "summary": "Announces Figure's Helix 02, a pixels-to-whole-body autonomy stack that unifies locomotion and manipulation with a single full-body visuomotor policy backed by a learned whole-body controller trained on large-scale human motion data.",
    "sources": [
      {
        "type": "website",
        "title": "Figure AI news release",
        "url": "https://www.figure.ai/news/helix-02"
      }
    ],
    "tags": [
      "Humanoid Robotics",
      "Vision-Language-Action",
      "Loco-Manipulation",
      "Full-Body Control"
    ],
    "last_reviewed": "27 Jan 2026"
  },
  {
    "id": 117,
    "title": "LingBot-VLA: A Practical VLA Foundation Model",
    "year": "26 Jan 2026",
    "summary": "Introduces LingBot-VLA, a practical vision-language-action foundation model trained on roughly 20,000 hours of real-world bimanual robot data spanning nine robot configurations, with evaluations across three platforms and 100 tasks per platform. The project highlights scalable training and releases code, models, and benchmark datasets to support downstream robotics research.",
    "sources": [
      {
        "type": "website",
        "title": "LingBot-VLA project page",
        "url": "https://technology.robbyant.com/lingbot-vla"
      },
      {
        "type": "paper",
        "title": "Tech report",
        "url": "https://arxiv.org/abs/2601.18692"
      },
      {
        "type": "code",
        "title": "Source code",
        "url": "https://github.com/Robbyant/lingbot-vla"
      },
      {
        "type": "model",
        "title": "Hugging Face collection",
        "url": "https://huggingface.co/collections/robbyant/lingbot-vla"
      },
      {
        "type": "dataset",
        "title": "GM-100 dataset",
        "url": "https://huggingface.co/datasets/robbyant/lingbot-GM-100"
      }
    ],
    "tags": [
      "Vision-Language-Action",
      "Foundation Model",
      "Robotic Manipulation",
      "Real-World Data",
      "Benchmark"
    ],
    "last_reviewed": "26 Jan 2026"
  },
  {
    "id": 118,
    "title": "The Shawshank Redemption of Embodied AI: Understanding and Benchmarking Indirect Environmental Jailbreaks",
    "year": "20 Nov 2025",
    "summary": "Introduces the first study of indirect environmental jailbreaks for embodied AI agents, where attackers influence the environment rather than issuing direct prompts, and provides a benchmark to evaluate how VLM-based agents respond to these indirect jailbreaking cues.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2511.16347"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2511.16347"
      }
    ],
    "tags": [
      "Embodied Agents",
      "Vision-Language Models",
      "Jailbreak",
      "Safety",
      "Benchmark"
    ],
    "last_reviewed": "20 Nov 2025"
  },
  {
    "id": 119,
    "title": "DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation",
    "year": "09 Dec 2025",
    "summary": "DynamicVLA enables open-ended dynamic object manipulation by pairing a compact 0.4B vision-language model with low-latency continuous inference and latent-aware action streaming, evaluated at scale through the new DOM benchmark in simulation and real-world settings.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2601.22153"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2601.22153"
      },
      {
        "type": "website",
        "title": "Project page",
        "url": "https://www.infinitescript.com/project/dynamic-vla/"
      },
      {
        "type": "code",
        "title": "Source code",
        "url": "https://github.com/hzxie/DynamicVLA"
      },
      {
        "type": "dataset",
        "title": "DOM dataset",
        "url": "https://huggingface.co/datasets/hzxie/DOM"
      }
    ],
    "tags": [
      "Vision-Language-Action",
      "Dynamic Object Manipulation",
      "Robotic Manipulation",
      "Benchmark"
    ],
    "last_reviewed": "09 Dec 2025"
  },
  {
    "id": 120,
    "title": "Vision Encoders in Vision-Language Models: A Survey",
    "year": "05 Jan 2026",
    "summary": "Surveys vision encoders across 70+ vision-language models and finds training methodology, data curation, and feature objectives matter more than encoder size, while native resolution handling and multi-encoder fusion improve document understanding and feature coverage.",
    "sources": [
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://jina.ai/vision-encoder-survey.pdf"
      }
    ],
    "tags": [
      "Survey",
      "Vision Encoders",
      "Vision-Language Models"
    ],
    "last_reviewed": "31 Jan 2026"
  },
  {
    "id": 121,
    "title": "UnifoLM-VLA-0: A Vision-Language-Action (VLA) Framework under UnifoLM Family",
    "year": "29 Jan 2026",
    "summary": "UnifoLM-VLA-0 is a vision-language-action model in the UnifoLM series for general-purpose humanoid robot manipulation, with continued pre-training on robot manipulation data to strengthen spatial reasoning and support broad task generalization in real-robot settings.",
    "sources": [
      {
        "type": "website",
        "title": "Project page",
        "url": "https://unigen-x.github.io/unifolm-vla.github.io"
      },
      {
        "type": "code",
        "title": "Source code",
        "url": "https://github.com/unitreerobotics/unifolm-vla"
      },
      {
        "type": "model",
        "title": "Model collection",
        "url": "https://huggingface.co/collections/unitreerobotics/unifolm-wma-0-68ca23027310c0ca0f34959c"
      },
      {
        "type": "dataset",
        "title": "Datasets",
        "url": "https://huggingface.co/unitreerobotics/datasets"
      }
    ],
    "tags": [
      "Vision-Language-Action",
      "Humanoid Robotics",
      "Robotic Manipulation",
      "Foundation Model"
    ],
    "last_reviewed": "29 Jan 2026"
  },
  {
    "id": 122,
    "title": "ACG: Action Coherence Guidance for Flow-based VLA models",
    "year": "25 Oct 2025",
    "summary": "Introduces Action Coherence Guidance (ACG), a training-free test-time guidance method for flow-matching Vision-Language-Action policies that improves action coherence by steering sampling away from incoherent action generation, boosting stability and success rates across RoboCasa, DexMimicGen, and real-world SO-101 manipulation tasks.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2510.22201"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2510.22201"
      },
      {
        "type": "website",
        "title": "Project page",
        "url": "https://davian-robotics.github.io/ACG/"
      },
      {
        "type": "code",
        "title": "Source code",
        "url": "https://github.com/davian-robotics/ACG"
      }
    ],
    "tags": [
      "Vision-Language-Action",
      "Flow Matching",
      "Action Coherence",
      "Robot Manipulation",
      "Test-Time Guidance"
    ],
    "last_reviewed": "25 Oct 2025"
  },
  {
    "id": 123,
    "title": "Humanity's Last Machine",
    "year": "03 Feb 2026",
    "summary": "Interactive resource cataloging humanoid robotics hardware components, supplier landscape, and related ecosystem context for tracking progress in humanoid robotics.",
    "sources": [
      {
        "type": "website",
        "title": "Humanity's Last Machine",
        "url": "https://www.humanityslastmachine.com/"
      }
    ],
    "tags": [
      "Humanoid Robotics",
      "Hardware",
      "Landscape",
      "Resource"
    ],
    "last_reviewed": "03 Feb 2026"
  },
  {
    "id": 124,
    "title": "HumanX: Toward Agile and Generalizable Humanoid Interaction Skills from Human Videos",
    "year": "02 Feb 2026",
    "summary": "Introduces HumanX, a full-stack framework that turns human videos into diverse robot interaction data and trains generalizable humanoid skills without task-specific rewards, demonstrating zero-shot transfer of 10 interaction skills to a physical Unitree G1 across sports and cargo tasks.",
    "sources": [
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2602.02473"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2602.02473"
      }
    ],
    "tags": [
      "Humanoid Robotics",
      "Imitation Learning",
      "Human Video",
      "Interaction Skills",
      "Data Generation"
    ],
    "last_reviewed": "04 Feb 2026"
  },
  {
    "id": 125,
    "title": "HUSKY: Humanoid Skateboarding System via Physics-Aware Whole-Body Control",
    "year": "05 Feb 2026",
    "summary": "Presents HUSKY, a learning-based framework for humanoid skateboarding that models humanoid-skateboard coupling and uses physics-aware whole-body control with adversarial motion priors to deliver stable, agile maneuvering on a Unitree G1 platform.",
    "sources": [
      {
        "type": "website",
        "title": "HUSKY project page",
        "url": "https://husky-humanoid.github.io"
      },
      {
        "type": "paper",
        "title": "arXiv",
        "url": "https://arxiv.org/abs/2602.03205"
      },
      {
        "type": "pdf",
        "title": "PDF",
        "url": "https://arxiv.org/pdf/2602.03205"
      }
    ],
    "tags": [
      "Humanoid Robotics",
      "Whole-Body Control",
      "Skateboarding",
      "Physics-Aware Control"
    ],
    "last_reviewed": "05 Feb 2026"
  }
]
